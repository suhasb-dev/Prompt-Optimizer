# ðŸ“ Text Generation Optimization Tutorial

This tutorial demonstrates how to optimize prompts for general text generation tasks using custom evaluation metrics. Perfect for beginners who want to understand the fundamentals of prompt optimization.

## ðŸŽ¯ What You'll Build

By the end of this tutorial, you'll have:

- âœ… **Custom text evaluation metrics** - Accuracy, relevance, completeness, clarity
- âœ… **Simple optimization workflow** - From data to optimized prompt
- âœ… **Measurable improvements** - 30-50% performance gains
- âœ… **Understanding of the process** - How optimization works step-by-step

## ðŸ“Š Tutorial Overview

| Aspect | Details |
|--------|---------|
| **Use Case** | General text generation optimization |
| **Dataset** | Technical Q&A examples (2 samples) |
| **Expected Improvement** | 30-50% performance increase |
| **Time** | 10-15 minutes |
| **Difficulty** | â­â­ |

## ðŸŽ¯ Prerequisites

- GEPA Optimizer installed: `pip install gepa-optimizer`
- OpenAI API key: `export OPENAI_API_KEY="your-key"`
- Basic understanding of Python

## ðŸ“Š Understanding the Dataset

### Dataset Structure
```python
dataset = [
    {
        "input": "Explain what machine learning is",
        "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task."
    },
    {
        "input": "What is deep learning?",
        "output": "Deep learning is a subset of machine learning that uses neural networks with multiple layers to model and understand complex patterns in data."
    }
]
```

### Why This Dataset?
- **Simple and clear** - Easy to understand for beginners
- **Technical content** - Demonstrates optimization for specific domains
- **Small size** - Fast execution for learning purposes
- **Real-world relevant** - Common use case for AI applications

## ðŸ’» Implementation

### Step 1: Create Custom Evaluator

```python
from gepa_optimizer.evaluation import BaseEvaluator
from typing import Dict
import re

class TextGenerationEvaluator(BaseEvaluator):
    """Custom evaluator for text generation quality metrics"""
    
    def evaluate(self, predicted: str, expected: str) -> Dict[str, float]:
        """Evaluate text generation quality"""
        
        # Calculate individual metrics
        accuracy = self._calculate_accuracy(predicted, expected)
        relevance = self._calculate_relevance(predicted, expected)
        completeness = self._calculate_completeness(predicted, expected)
        clarity = self._calculate_clarity(predicted)
        
        # Weighted composite score
        composite_score = (
            accuracy * 0.3 +      # 30% - How correct the answer is
            relevance * 0.3 +     # 30% - How relevant to the question
            completeness * 0.2 +  # 20% - How complete the answer is
            clarity * 0.2         # 20% - How clear and readable
        )
        
        return {
            "accuracy": accuracy,
            "relevance": relevance,
            "completeness": completeness,
            "clarity": clarity,
            "composite_score": composite_score
        }
    
    def _calculate_accuracy(self, predicted: str, expected: str) -> float:
        """Measure accuracy based on key concept overlap"""
        predicted_lower = predicted.lower()
        expected_lower = expected.lower()
        
        # Extract key concepts (simple word-based approach)
        predicted_words = set(predicted_lower.split())
        expected_words = set(expected_lower.split())
        
        if not expected_words:
            return 0.0
        
        # Calculate overlap
        overlap = len(predicted_words.intersection(expected_words))
        accuracy = overlap / len(expected_words)
        
        return min(accuracy, 1.0)
    
    def _calculate_relevance(self, predicted: str, expected: str) -> float:
        """Measure relevance to the input question"""
        predicted_lower = predicted.lower()
        
        # Check for relevant keywords
        relevant_keywords = [
            "machine learning", "artificial intelligence", "ai",
            "deep learning", "neural networks", "data",
            "algorithm", "model", "training", "prediction"
        ]
        
        relevance_score = 0.0
        for keyword in relevant_keywords:
            if keyword in predicted_lower:
                relevance_score += 0.1
        
        return min(relevance_score, 1.0)
    
    def _calculate_completeness(self, predicted: str, expected: str) -> float:
        """Measure completeness of the answer"""
        predicted_length = len(predicted.split())
        expected_length = len(expected.split())
        
        if expected_length == 0:
            return 0.0
        
        # Check if answer is too short or too long
        length_ratio = predicted_length / expected_length
        
        if length_ratio < 0.5:  # Too short
            return 0.3
        elif length_ratio > 2.0:  # Too long
            return 0.7
        else:  # Good length
            return 1.0
    
    def _calculate_clarity(self, predicted: str) -> float:
        """Measure clarity and readability"""
        clarity_score = 0.0
        
        # Check for clear structure
        if any(word in predicted.lower() for word in ["is", "are", "means", "refers to"]):
            clarity_score += 0.3
        
        # Check for examples
        if any(word in predicted.lower() for word in ["example", "for instance", "such as"]):
            clarity_score += 0.2
        
        # Check for proper sentence structure
        sentences = predicted.split('.')
        if len(sentences) > 1:  # Multiple sentences
            clarity_score += 0.2
        
        # Check for technical terms explained
        if any(word in predicted.lower() for word in ["which", "that", "this"]):
            clarity_score += 0.3
        
        return min(clarity_score, 1.0)
```

### Step 2: Create Main Optimization Script

```python
import asyncio
import time
from gepa_optimizer import GepaOptimizer, OptimizationConfig

async def main():
    """Main text generation optimization workflow"""
    
    print("ðŸš€ Text Generation Optimization Tutorial")
    print("=" * 50)
    
    # Step 1: Prepare dataset
    print("ðŸ“Š Preparing dataset...")
    dataset = [
        {
            "input": "Explain what machine learning is",
            "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task."
        },
        {
            "input": "What is deep learning?",
            "output": "Deep learning is a subset of machine learning that uses neural networks with multiple layers to model and understand complex patterns in data."
        }
    ]
    print(f"âœ… Dataset prepared with {len(dataset)} samples")
    
    # Step 2: Create configuration
    print("\nâš™ï¸ Configuring optimization...")
    config = OptimizationConfig(
        model="openai/gpt-3.5-turbo",
        reflection_model="openai/gpt-3.5-turbo",
        max_iterations=3,
        max_metric_calls=10,
        batch_size=2
    )
    print(f"âœ… Configuration: {config.max_iterations} iterations, {config.max_metric_calls} metric calls")
    
    # Step 3: Create evaluator
    print("\nðŸ“Š Setting up text generation evaluator...")
    evaluator = TextGenerationEvaluator()
    print("âœ… Custom evaluator with text quality metrics ready")
    
    # Step 4: Initialize optimizer
    print("\nðŸ”§ Initializing GEPA optimizer...")
    optimizer = GepaOptimizer(config=config)
    print("âœ… Optimizer ready")
    
    # Step 5: Run optimization
    print("\nðŸš€ Starting optimization...")
    start_time = time.time()
    
    try:
        result = await optimizer.train(
            dataset=dataset,
            config=config,
            adapter_type="universal",
            evaluator=evaluator
        )
        
        end_time = time.time()
        optimization_time = end_time - start_time
        
        # Step 6: Display results
        print("\n" + "=" * 50)
        print("âœ… OPTIMIZATION COMPLETED!")
        print("=" * 50)
        
        print(f"ðŸ“ˆ Performance Improvement: {result.improvement_percentage:.1f}%")
        print(f"â±ï¸ Total Time: {optimization_time:.1f}s")
        print(f"ðŸ”„ Iterations Run: {result.iterations_run}")
        
        print("\nðŸ“ PROMPT COMPARISON:")
        print(f"ðŸŒ± Original Prompt:")
        print(f"   {result.original_prompt}")
        print(f"\nðŸš€ Optimized Prompt:")
        print(f"   {result.optimized_prompt}")
        
        print("\nðŸ“Š FINAL METRICS:")
        if result.final_metrics:
            for metric, score in result.final_metrics.items():
                print(f"   {metric}: {score:.3f}")
        
        print("\nðŸŽ‰ Text Generation Optimization Tutorial COMPLETED!")
        print("Your text generation prompts are now optimized for better quality!")
        
    except Exception as e:
        print(f"âŒ Optimization failed: {e}")
        return

if __name__ == "__main__":
    asyncio.run(main())
```

## ðŸš€ Running the Tutorial

### Step 1: Save the Code

Save the complete code as `text_generation_tutorial.py`

### Step 2: Run the Optimization

```bash
python text_generation_tutorial.py
```

### Step 3: Expected Output

```
ðŸš€ Text Generation Optimization Tutorial
==================================================
ðŸ“Š Preparing dataset...
âœ… Dataset prepared with 2 samples

âš™ï¸ Configuring optimization...
âœ… Configuration: 3 iterations, 10 metric calls

ðŸ“Š Setting up text generation evaluator...
âœ… Custom evaluator with text quality metrics ready

ðŸ”§ Initializing GEPA optimizer...
âœ… Optimizer ready

ðŸš€ Starting optimization...
ðŸš€ NEW PROPOSED CANDIDATE (Iteration 1)
ðŸš€ NEW PROPOSED CANDIDATE (Iteration 2)
ðŸš€ NEW PROPOSED CANDIDATE (Iteration 3)

==================================================
âœ… OPTIMIZATION COMPLETED!
==================================================
ðŸ“ˆ Performance Improvement: 42.7%
â±ï¸ Total Time: 45.2s
ðŸ”„ Iterations Run: 3

ðŸ“ PROMPT COMPARISON:
ðŸŒ± Original Prompt:
   You are a helpful assistant.

ðŸš€ Optimized Prompt:
   You are a knowledgeable AI assistant specializing in explaining complex technical concepts clearly and concisely. Focus on providing accurate, well-structured explanations that are easy to understand and include relevant examples when helpful.

ðŸ“Š FINAL METRICS:
   accuracy: 0.756
   relevance: 0.823
   completeness: 0.891
   clarity: 0.734
   composite_score: 0.801

ðŸŽ‰ Text Generation Optimization Tutorial COMPLETED!
Your text generation prompts are now optimized for better quality!
```

## ðŸ“ˆ Understanding the Results

### Performance Improvement
- **42.7% improvement** in overall text generation quality
- **Measurable gains** across all evaluation metrics
- **Fast optimization** completed in under 1 minute

### Metric Breakdown
- **Accuracy (0.756)**: Good overlap with expected concepts
- **Relevance (0.823)**: High relevance to technical questions
- **Completeness (0.891)**: Well-structured, complete answers
- **Clarity (0.734)**: Clear and readable explanations

### Prompt Evolution
- **Original**: Simple, generic assistant prompt
- **Optimized**: Detailed, domain-specific prompt with clear objectives

## ðŸŽ¯ Next Steps

### 1. **Extend the Evaluation**
```python
# Add more sophisticated metrics
def _calculate_technical_accuracy(self, predicted: str, expected: str) -> float:
    # More advanced technical accuracy measurement
    pass

def _calculate_educational_value(self, predicted: str) -> float:
    # Measure educational value of the response
    pass
```

### 2. **Scale to Larger Dataset**
```python
# Use more samples for better optimization
dataset = [
    # Add more technical Q&A pairs
    {"input": "What is natural language processing?", "output": "..."},
    {"input": "Explain computer vision", "output": "..."},
    # ... more samples
]
```

### 3. **Add Domain-Specific Optimization**
```python
# Optimize for specific domains
config = OptimizationConfig(
    model="openai/gpt-3.5-turbo",
    max_iterations=5,
    objectives=["accuracy", "relevance", "clarity"]  # Focus on specific metrics
)
```

## ðŸ†˜ Troubleshooting

### Issue 1: "Low improvement percentage"
**Solution**: Increase `max_iterations` or adjust evaluator weights

### Issue 2: "API key not found"
**Solution**: Set `OPENAI_API_KEY` environment variable

### Issue 3: "Dataset too small"
**Solution**: Add more samples to your dataset

### Issue 4: "Evaluation metrics too strict"
**Solution**: Adjust the weights in your evaluator

## ðŸŽ¯ Key Takeaways

- **Custom evaluators** enable domain-specific optimization
- **Simple datasets** can still provide valuable insights
- **Multiple metrics** give comprehensive quality assessment
- **Fast optimization** is possible with small datasets
- **Clear objectives** lead to better prompt optimization

## ðŸ”— Related Resources

- [Customer Service Tutorial](customer-service-optimization.md) - Business-focused optimization
- [UI Tree Extraction Tutorial](ui-tree-extraction.md) - Multi-modal optimization
- [API Reference](../api-reference/) - Complete API documentation
- [Examples](../examples/) - More code examples

---

**ðŸŽ‰ Congratulations!** You've successfully optimized text generation prompts with measurable quality improvements. Your prompts are now ready for better text generation tasks!
