# ğŸš€ GEPA Universal Prompt Optimizer

[![PyPI version](https://badge.fury.io/py/gepa-optimizer.svg)](https://badge.fury.io/py/gepa-optimizer)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Tests](https://github.com/suhasb-dev/Prompt-Optimizer/workflows/Tests/badge.svg)](https://github.com/suhasb-dev/Prompt-Optimizer/actions)

> **A production-ready Python library for universal prompt optimization using the GEPA (Generative Evaluation and Prompt Adaptation) framework. Built for developers who need reliable, scalable prompt optimization with comprehensive evaluation metrics.**

## ğŸ¯ What is GEPA Optimizer?

GEPA Optimizer is a sophisticated framework that automatically improves prompts for Large Language Models (LLMs) by iteratively evaluating and refining them based on performance metrics. Think of it as "AutoML for prompts" - it takes your initial prompt and dataset, then uses advanced optimization techniques to create a better-performing prompt.

### Key Capabilities

- **ğŸ”„ Universal Prompt Optimization**: Works with any LLM provider (OpenAI, Anthropic, Google, Hugging Face)
- **ğŸ‘ï¸ Multi-Modal Support**: Optimize prompts for vision-capable models (GPT-4V, Claude-3, Gemini)
- **ğŸ“Š Advanced Evaluation**: Comprehensive metrics for UI tree extraction and general prompt performance
- **ğŸ­ Production Ready**: Enterprise-grade reliability with async support, error handling, and monitoring
- **âš™ï¸ Flexible Configuration**: Easy-to-use configuration system for any optimization scenario
- **ğŸ’° Cost Optimization**: Built-in budget controls and cost estimation
- **ğŸ¨ UI Tree Extraction**: Specialized for optimizing UI interaction and screen understanding tasks
- **ğŸ”§ Extensible Architecture**: Create custom evaluators and adapters for any use case

## ğŸš€ Quick Start

### Installation

```bash
pip install gepa-optimizer
```

### Basic Usage

```python
import asyncio
from gepa_optimizer import GepaOptimizer, OptimizationConfig

async def optimize_prompt():
    # Configure your optimization
    config = OptimizationConfig(
        model="openai/gpt-4o",                    # Your target model
        reflection_model="openai/gpt-4o",         # Model for self-reflection
        max_iterations=10,                        # Budget: optimization rounds
        max_metric_calls=50,                      # Budget: evaluation calls
        batch_size=4                              # Batch size for evaluation
    )
    
    # Initialize optimizer
    optimizer = GepaOptimizer(config=config)
    
    # Your training data
    dataset = [
        {"input": "What is artificial intelligence?", "output": "AI is a field of computer science..."},
        {"input": "Explain machine learning", "output": "Machine learning is a subset of AI..."},
        {"input": "What are neural networks?", "output": "Neural networks are computing systems..."}
    ]
    
    # Optimize your prompt
    result = await optimizer.train(
        seed_prompt="You are a helpful AI assistant that explains technical concepts clearly.",
        dataset=dataset
    )
    
    print(f"âœ… Optimization completed!")
    print(f"ğŸ“ˆ Performance improvement: {result.improvement_percent:.2f}%")
    print(f"ğŸ¯ Optimized prompt: {result.prompt}")
    print(f"â±ï¸ Time taken: {result.optimization_time:.2f}s")

# Run the optimization
asyncio.run(optimize_prompt())
```

### UI Tree Extraction (Vision Models)

```python
import asyncio
from gepa_optimizer import GepaOptimizer, OptimizationConfig

async def optimize_ui_prompt():
    config = OptimizationConfig(
        model="openai/gpt-4o",                    # Vision-capable model
        reflection_model="openai/gpt-4o",
        max_iterations=15,
        max_metric_calls=75,
        batch_size=3
    )
    
    optimizer = GepaOptimizer(config=config)
    
    # UI screenshot data with images and JSON trees
    dataset = {
        'json_dir': 'json_tree',           # Directory with ground truth JSON files
        'screenshots_dir': 'screenshots',  # Directory with UI screenshots
        'type': 'ui_tree_dataset'          # Dataset type
    }
    
    result = await optimizer.train(
        seed_prompt="Analyze the UI screenshot and extract all elements as JSON.",
        dataset=dataset
    )
    
    print(f"ğŸ¯ Optimized UI prompt: {result.prompt}")
    print(f"ğŸ“Š Improvement: {result.improvement_percent:.2f}%")

asyncio.run(optimize_ui_prompt())
```

### Universal Custom Use Cases

```python
import asyncio
from gepa_optimizer import (
    GepaOptimizer, OptimizationConfig, ModelConfig,
    BaseEvaluator, BaseLLMClient, VisionLLMClient
)

class CustomEvaluator(BaseEvaluator):
    """Your custom evaluation logic for any use case"""
    
    def evaluate(self, predicted: str, expected: str) -> Dict[str, float]:
        # Implement your custom metrics
        accuracy = calculate_accuracy(predicted, expected)
        relevance = calculate_relevance(predicted, expected)
        
        return {
            "accuracy": accuracy,
            "relevance": relevance,
            "composite_score": (accuracy + relevance) / 2
        }

async def custom_optimization():
    # Create your custom components
    llm_client = VisionLLMClient(
        provider="openai",
        model_name="gpt-4o",
        api_key="your-api-key"
    )
    evaluator = CustomEvaluator()
    
    # Configure optimization
    config = OptimizationConfig(
        model="openai/gpt-4o",
        reflection_model="openai/gpt-4o",
        max_iterations=10,
        max_metric_calls=50
    )
    
    # Use universal adapter
    optimizer = GepaOptimizer(
        config=config,
        adapter_type="universal",
        llm_client=llm_client,
        evaluator=evaluator
    )
    
    # Your custom dataset
    dataset = [
        {"input": "Your input", "output": "Expected output"},
        # ... more examples
    ]
    
    result = await optimizer.train(
        seed_prompt="Your initial prompt",
        dataset=dataset
    )
    
    print(f"ğŸ¯ Optimized prompt: {result.prompt}")

asyncio.run(custom_optimization())
```

## ğŸ“š Comprehensive Examples

### 1. Multi-Provider Setup

```python
from gepa_optimizer import OptimizationConfig, ModelConfig

# Using different providers for main and reflection models
config = OptimizationConfig(
    model=ModelConfig(
        provider="openai",
        model_name="gpt-4o",
        api_key="your-openai-key",
        temperature=0.7
    ),
    reflection_model=ModelConfig(
        provider="anthropic",
        model_name="claude-3-opus-20240229",
        api_key="your-anthropic-key",
        temperature=0.5
    ),
    max_iterations=30,
    max_metric_calls=200,
    batch_size=6
)
```

### 2. Budget and Cost Control

```python
config = OptimizationConfig(
    model="openai/gpt-4o",
    reflection_model="openai/gpt-3.5-turbo",  # Cheaper reflection model
    max_iterations=20,
    max_metric_calls=100,
    batch_size=4,
    max_cost_usd=50.0,                        # Budget limit
    timeout_seconds=1800                      # 30 minute timeout
)

# Check estimated costs before running
cost_estimate = config.get_estimated_cost()
print(f"ğŸ’° Estimated cost range: {cost_estimate}")
```

### 3. Advanced Configuration

```python
config = OptimizationConfig(
    model="openai/gpt-4o",
    reflection_model="openai/gpt-4o",
    max_iterations=50,
    max_metric_calls=300,
    batch_size=8,
    
    # Advanced settings
    early_stopping=True,
    learning_rate=0.02,
    multi_objective=True,
    objectives=["accuracy", "relevance", "clarity"],
    
    # Data settings
    train_split_ratio=0.85,
    min_dataset_size=5,
    
    # Performance settings
    parallel_evaluation=True,
    use_cache=True
)
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GEPA Universal Prompt Optimizer              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  User Interface Layer                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  GepaOptimizer  â”‚  â”‚ optimize_prompt â”‚                      â”‚
â”‚  â”‚   (Main API)    â”‚  â”‚   (Convenience) â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Core Processing Layer                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚CustomGepaAdapterâ”‚  â”‚UniversalAdapter â”‚  â”‚UniversalConverterâ”‚ â”‚
â”‚  â”‚  (UI Tree)      â”‚  â”‚  (Universal)    â”‚  â”‚ (Data Converter)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Evaluation & LLM Layer                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚UITreeEvaluator  â”‚  â”‚BaseEvaluator    â”‚  â”‚VisionLLMClient  â”‚ â”‚
â”‚  â”‚ (UI Metrics)    â”‚  â”‚ (Custom Metrics)â”‚  â”‚  (LLM Interface)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data & Utility Layer                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   DataLoader    â”‚  â”‚  APIKeyManager  â”‚  â”‚    Exceptions   â”‚ â”‚
â”‚  â”‚ (File Loading)  â”‚  â”‚ (Key Management)â”‚  â”‚   (Error Handling)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
gepa-optimizer/
â”œâ”€â”€ gepa_optimizer/              # Main package
â”‚   â”œâ”€â”€ core/                   # Core optimization engine
â”‚   â”‚   â”œâ”€â”€ optimizer.py        # Main GepaOptimizer class
â”‚   â”‚   â”œâ”€â”€ custom_adapter.py   # UI tree GEPA integration
â”‚   â”‚   â”œâ”€â”€ universal_adapter.py # Universal GEPA integration
â”‚   â”‚   â”œâ”€â”€ base_adapter.py     # Base adapter class
â”‚   â”‚   â””â”€â”€ result.py           # Result processing
â”‚   â”œâ”€â”€ models/                 # Configuration and data models
â”‚   â”‚   â”œâ”€â”€ config.py           # Configuration classes
â”‚   â”‚   â”œâ”€â”€ dataset.py          # Dataset models
â”‚   â”‚   â””â”€â”€ result.py           # Result models
â”‚   â”œâ”€â”€ data/                   # Data conversion and validation
â”‚   â”‚   â”œâ”€â”€ converters.py       # Universal data converter
â”‚   â”‚   â”œâ”€â”€ loaders.py          # File loading utilities
â”‚   â”‚   â””â”€â”€ validators.py       # Data validation
â”‚   â”œâ”€â”€ evaluation/             # Evaluation metrics and analysis
â”‚   â”‚   â”œâ”€â”€ ui_evaluator.py     # UI tree evaluation metrics
â”‚   â”‚   â””â”€â”€ base_evaluator.py   # Base evaluator class
â”‚   â”œâ”€â”€ llms/                   # LLM client integrations
â”‚   â”‚   â”œâ”€â”€ vision_llm.py       # Multi-modal LLM client
â”‚   â”‚   â””â”€â”€ base_llm.py         # Base LLM client class
â”‚   â”œâ”€â”€ utils/                  # Utilities and helpers
â”‚   â”‚   â”œâ”€â”€ api_keys.py         # API key management
â”‚   â”‚   â”œâ”€â”€ exceptions.py       # Custom exceptions
â”‚   â”‚   â”œâ”€â”€ helpers.py          # Helper functions
â”‚   â”‚   â”œâ”€â”€ logging.py          # Logging utilities
â”‚   â”‚   â””â”€â”€ metrics.py          # Metrics utilities
â”‚   â””â”€â”€ cli.py                  # Command-line interface
â”œâ”€â”€ tests/                      # Test suite
â”œâ”€â”€ examples/                   # Usage examples
â”œâ”€â”€ docs/                       # Documentation
â””â”€â”€ requirements.txt            # Dependencies
```

## ğŸ”§ Installation & Setup

### Prerequisites

- Python 3.8 or higher
- API keys for your chosen LLM providers

### Environment Setup

1. **Install the package:**
   ```bash
   pip install gepa-optimizer
   ```

2. **Set up environment variables:**
   ```bash
   # For OpenAI
   export OPENAI_API_KEY="your-openai-api-key"
   
   # For Anthropic
   export ANTHROPIC_API_KEY="your-anthropic-api-key"
   
   # For Google Gemini
   export GOOGLE_API_KEY="your-google-api-key"
   
   # For Hugging Face
   export HUGGINGFACE_API_KEY="your-hf-api-key"
   ```

3. **Or use a .env file:**
   ```env
   OPENAI_API_KEY=your-openai-api-key
   ANTHROPIC_API_KEY=your-anthropic-api-key
   GOOGLE_API_KEY=your-google-api-key
   HUGGINGFACE_API_KEY=your-hf-api-key
   ```

## ğŸ“– Documentation

| Resource | Description |
|----------|-------------|
| [ğŸ—ï¸ System Architecture](docs/architecture/system-overview.md) | Complete system architecture and design patterns |
| [ğŸ“ Examples](examples/) | Practical examples and tutorials |
| [ğŸ§ª Test Files](#testing--validation) | Comprehensive test suite documentation |
| [ğŸš€ Quick Start](#quick-start) | Get started in 5 minutes |
| [âš™ï¸ Configuration](#configuration) | Advanced configuration options |

## ğŸ¯ Use Cases

- **ğŸ¤– Chatbot Optimization**: Improve response quality and consistency
- **ğŸ–¥ï¸ UI Automation**: Optimize prompts for screen understanding and interaction
- **ğŸ“ Content Generation**: Enhance prompts for creative writing, summaries, etc.
- **ğŸ’» Code Generation**: Optimize prompts for programming tasks
- **ğŸ‘ï¸ Multi-modal Applications**: Vision + text prompt optimization
- **ğŸ¯ Domain-Specific Tasks**: Fine-tune prompts for specialized domains
- **ğŸ“Š Data Analysis**: Optimize prompts for data interpretation and analysis
- **ğŸ” Search & Retrieval**: Improve search query optimization
- **ğŸ“š Educational Content**: Optimize prompts for learning and teaching
- **ğŸ¨ Creative Writing**: Enhance prompts for creative and artistic tasks


*Benchmarks run on standard text classification tasks with UI tree extraction*

## ğŸ”’ Security & Privacy

- **ğŸ” API Key Security**: Keys are never logged or stored in plain text
- **ğŸ›¡ï¸ Data Privacy**: Your data never leaves your control
- **ğŸ”’ Secure Connections**: All API calls use HTTPS/TLS encryption
- **ğŸ“‹ Audit Trail**: Complete logging of optimization process
- **ğŸ¢ Enterprise Ready**: SOC 2 compliance ready architecture

## ğŸ§ª Testing & Validation

The GEPA Universal Prompt Optimizer includes comprehensive test suites that demonstrate the library's capabilities across different use cases. Each test file showcases specific features and provides real-world examples of optimization.

### ğŸ§ª Test Suite Overview

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=gepa_optimizer

# Run specific test categories
pytest tests/unit/          # Unit tests
pytest tests/integration/   # Integration tests

# Run individual test files
python test_customer_service_optimization.py
python test_text_generation.py
python test_ui_optimization.py
```

### ğŸ“‹ Test Files Documentation

#### 1. ğŸ¯ Customer Service Optimization Test
**File:** `test_customer_service_optimization.py`

**Purpose:** Demonstrates universal adapter with custom business-specific evaluation metrics for customer service applications.

**What it tests:**
- âœ… **Universal Adapter**: Shows how the universal adapter works with any use case
- âœ… **Custom Evaluation Metrics**: Business-specific metrics (helpfulness, empathy, solution focus, professionalism)
- âœ… **Real Dataset**: Uses actual customer service data from Bitext dataset (27K responses)
- âœ… **Multi-Modal Support**: Works with text-only models for customer service optimization
- âœ… **Measurable Improvements**: Shows concrete performance gains

**Key Features Demonstrated:**
```python
class CustomerServiceEvaluator(BaseEvaluator):
    """Custom evaluator with business-specific metrics"""
    
    def evaluate(self, predicted: str, expected: str) -> Dict[str, float]:
        return {
            "helpfulness": self._calculate_helpfulness(predicted, expected),
            "empathy": self._calculate_empathy(predicted),
            "solution_focus": self._calculate_solution_focus(predicted),
            "professionalism": self._calculate_professionalism(predicted),
            "composite_score": weighted_average
        }
```

**Dataset:** 
- **Source**: Bitext Customer Support Training Dataset (27K responses)
- **Format**: CSV with columns: flags, instruction, category, intent, response
- **Categories**: ACCOUNT, ORDER, REFUND, CONTACT, INVOICE
- **Sample Size**: 50 interactions for optimization

**Expected Results:**
- **Improvement**: 40-70% performance increase
- **Prompt Evolution**: From simple 83-character prompt to detailed 2,000+ character guidelines
- **Iterations**: 4-5 optimization iterations
- **Time**: 2-5 minutes depending on configuration

**Run the test:**
```bash
python test_customer_service_optimization.py
```

---

#### 2. ğŸ“ Text Generation Optimization Test
**File:** `test_text_generation.py`

**Purpose:** Demonstrates universal implementation with custom evaluation metrics for general text generation tasks.

**What it tests:**
- âœ… **Custom Evaluation Logic**: User-defined metrics for text generation quality
- âœ… **Universal Adapter**: Works with any text generation use case
- âœ… **Multi-Metric Evaluation**: Accuracy, relevance, completeness, clarity
- âœ… **Weighted Scoring**: Configurable metric weights for different priorities
- âœ… **Real API Integration**: Full end-to-end optimization with actual LLM calls

**Key Features Demonstrated:**
```python
class TextGenerationEvaluator(BaseEvaluator):
    """Custom evaluator for text generation tasks"""
    
    def evaluate(self, predicted: str, expected: str) -> Dict[str, float]:
        return {
            "accuracy": self._calculate_accuracy(predicted, expected),
            "relevance": self._calculate_relevance(predicted, expected),
            "completeness": self._calculate_completeness(predicted, expected),
            "clarity": self._calculate_clarity(predicted),
            "composite_score": weighted_average
        }
```

**Dataset:**
- **Type**: Technical explanation dataset
- **Samples**: AI/ML concept explanations
- **Format**: Input-output pairs for technical Q&A
- **Size**: 2 detailed samples for testing

**Expected Results:**
- **Improvement**: 30-50% performance increase
- **Prompt Evolution**: From basic assistant prompt to detailed technical guide
- **Iterations**: 2-3 optimization iterations
- **Time**: 1-3 minutes

**Run the test:**
```bash
python test_text_generation.py
```

---

#### 3. ğŸ–¥ï¸ UI Tree Optimization Test
**File:** `test_ui_optimization.py`

**Purpose:** Demonstrates specialized UI tree extraction optimization using vision models and screenshot analysis.

**What it tests:**
- âœ… **Vision Model Integration**: Multi-modal optimization with image + text
- âœ… **UI Tree Extraction**: Specialized for screen understanding tasks
- âœ… **File-based Dataset**: Works with directories of images and JSON files
- âœ… **Legacy Adapter**: Uses the original UI-specific adapter
- âœ… **Screenshot Analysis**: Real UI screenshot processing

**Key Features Demonstrated:**
```python
# UI-specific dataset configuration
dataset = {
    'json_dir': 'json_tree',           # Ground truth JSON files
    'screenshots_dir': 'screenshots',  # UI screenshots
    'type': 'ui_tree_dataset'          # Dataset type
}

# Vision-capable model configuration
config = OptimizationConfig(
    model="openai/gpt-4o",             # Vision model
    reflection_model="openai/gpt-4o",
    max_iterations=10,
    max_metric_calls=20
)
```

**Dataset Requirements:**
- **Images**: Screenshots in `screenshots/` directory (.jpg, .jpeg, .png)
- **JSON**: Ground truth UI trees in `json_tree/` directory (.json)
- **Format**: Matching filenames between images and JSON files
- **Content**: UI screenshots with corresponding element trees

**Expected Results:**
- **Improvement**: 20-40% performance increase
- **Prompt Evolution**: From basic UI extraction to detailed element analysis
- **Iterations**: 5-10 optimization iterations
- **Time**: 5-15 minutes (longer due to vision processing)

**Run the test:**
```bash
python test_ui_optimization.py
```

### ğŸ¯ Test Results Summary

| Test File | Use Case | Adapter Type | Dataset | Expected Improvement | Time |
|-----------|----------|--------------|---------|---------------------|------|
| `test_customer_service_optimization.py` | Customer Service | Universal | Real CSV (50 samples) | 40-70% | 2-5 min |
| `test_text_generation.py` | Text Generation | Universal | Technical Q&A (2 samples) | 30-50% | 1-3 min |
| `test_ui_optimization.py` | UI Tree Extraction | Legacy | Screenshots + JSON | 20-40% | 5-15 min |

### ğŸ”§ Test Configuration

**Prerequisites for running tests:**
1. **API Keys**: Set `OPENAI_API_KEY` in environment or `.env` file
2. **Dependencies**: Install all requirements (`pip install -r requirements.txt`)
3. **Data Files**: For UI test, ensure `screenshots/` and `json_tree/` directories exist
4. **CSV Dataset**: For customer service test, place `Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv` in root directory

**Environment Setup:**
```bash
# Create .env file
echo "OPENAI_API_KEY=your-api-key-here" > .env

# Install dependencies
pip install -r requirements.txt

# Run tests
python test_customer_service_optimization.py
python test_text_generation.py
python test_ui_optimization.py
```

### ğŸ“Š Understanding Test Output

Each test provides detailed output including:

1. **Configuration Details**: Model settings, iteration counts, batch sizes
2. **Dataset Information**: Sample counts, data structure, validation results
3. **Optimization Progress**: Real-time iteration logs, score improvements
4. **Final Results**: Before/after prompt comparison, performance metrics
5. **Success Indicators**: Clear pass/fail status with detailed explanations

**Example Output:**
```
âœ… Optimization completed!
   - Status: completed
   - Iterations: 4
   - Time: 265.63s

ğŸ“ PROMPT COMPARISON
ğŸŒ± SEED PROMPT: "You are a customer service agent..."
ğŸš€ OPTIMIZED PROMPT: "You are a customer service agent specializing in..."

ğŸ‰ Customer Service Optimization Test PASSED!
```

### ğŸš€ Extending Tests

You can create your own test files by following the patterns in the existing tests:

1. **Create Custom Evaluator**: Inherit from `BaseEvaluator`
2. **Define Your Metrics**: Implement evaluation logic for your use case
3. **Prepare Dataset**: Format your data according to the expected structure
4. **Configure Optimization**: Set up `OptimizationConfig` with your parameters
5. **Run and Validate**: Execute optimization and verify results

This comprehensive test suite ensures that the GEPA Universal Prompt Optimizer works reliably across different domains and use cases, providing confidence in its production readiness.

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Make your changes and add tests**
4. **Run tests**: `pytest`
5. **Submit a pull request**

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **ğŸ› Issues**: [GitHub Issues](https://github.com/suhasb-dev/Prompt-Optimizer/issues)
- **ğŸ’¬ Discussions**: [GitHub Discussions](https://github.com/suhasb-dev/Prompt-Optimizer/discussions)
- **ğŸ“§ Email**: s8hasgrylls@gmail.com
- **ğŸ“š Documentation**: [Full Documentation](https://gepa-optimizer.readthedocs.io/)

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=suhasb-dev/Prompt-Optimizer&type=Date)](https://star-history.com/#suhasb-dev/Prompt-Optimizer&Date)

---

** Made with â¤ï¸ ** 